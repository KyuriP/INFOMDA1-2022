---
title: "07_ADacko"
author: "Aleksandra Dacko"
date: "10/26/2022"
output: html_document
---
# Practical 7

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
library(MASS)
library(splines)
library(ISLR)
library(tidyverse)
```
```{r}
set.seed(45)
```


## Prediction plot

Median housing prices in Boston do not have a linear relation with the proportion of low SES households. Today we are going to focus exclusively on prediction.

```{r Boston plot}
Boston %>% 
  ggplot(aes(x = lstat, y = medv)) +
  geom_point() +
  theme_minimal()
```

### 1.Create a function called pred_plot() that takes as input an lm object, which outputs the above plot but with a prediction line generated from the model object using the predict() method.

```{r}
pred_plot<-function(model){
  data<-Boston
  lstat_pred<-seq(min(data$lstat),max(data$lstat),length.out=500)
  pred_dat <-data.frame(medv=predict(model,newdata=tibble(lstat=lstat_pred)),lstat=lstat_pred)
  plot<-ggplot(aes(x = lstat, y = medv),data=Boston) +
  geom_point(colour="#979B8D")+theme_minimal()+geom_line(data=pred_dat,colour="#214E34",size=1)
  return(plot)
}
```
### 2.Create a linear regression object called lin_mod which models medv as a function of lstat. Check if your prediction plot works by running pred_plot(lin_mod). Do you see anything out of the ordinary with the predictions?

```{r}
lin_mod<-lm(medv~lstat,data = Boston)
pred_plot(lin_mod)
```
We can notice that the linear regression doesn't predict the valuse on the boundaries of the data correctly.

# Polynomial regression
### 3.Create another linear model pn3_mod, where you add the second and third-degree polynomial terms I(lstat^2) and I(lstat^3) to the formula. Create a pred_plot() with this model.
```{r}
pn3_mod<-lm(medv~lstat+I(lstat^2)+I(lstat^3),data = Boston)
pred_plot(pn3_mod)

```
This already looks more promissing.

### 4. Play around with the poly() function. What output does it generate with the arguments degree = 3 and raw = TRUE?

```{r}
#here we can just see values 1:5 squared and than cubed
poly(1:5, degree=3,raw = T)

```
### 5.Use the poly() function directly in the model formula to create a 3rd-degree polynomial regression predicting medv using lstat. Compare the prediction plot to the previous prediction plot you made. What happens if you change the poly() function to raw = FALSE?
```{r}
lm_model_poly3_T<-lm(medv~poly(lstat, degree=3,raw = T),data=Boston)
pred_plot(lm_model_poly3_T)
```
```{r}
lm_model_poly3_F<-lm(medv~poly(lstat, degree=3,raw = F),data=Boston)
pred_plot(lm_model_poly3_F)
```
Both plots seems to be the same. 
```{r}
head(predict(lm_model_poly3_F))
head(predict(lm_model_poly3_T))
```
```{r}
summary(lm_model_poly3_F)
summary(lm_model_poly3_T)
```
# Piecewise regression
### 6.Create a model called pw2_mod with one predictor: I(lstat <= median(lstat)). Create a pred_plot with this model. Use the coefficients in coef(pw2_mod) to find out what the predicted value for a low-lstat neighbourhood is.
```{r}
pw2_mod<-lm(medv~I(lstat <= median(lstat)),data=Boston)
coef(pw2_mod)
pred_plot(pw2_mod)
```
From here we can see that the predicted value for high-lstat equals to the intercept value and for the low-lstat part it is equal to 16.68+11.71=28.39.

### 7. Use the cut() function in the formula to generate a piecewise regression model called pw5_mod that contains 5 equally spaced sections. Again, plot the result using pred_plot.
```{r}
pw5_mod<-lm(medv~cut(lstat,breaks = 5),data=Boston)
pred_plot(pw5_mod)
```
```{r}
pw5_mod$coefficients
#here the piece with the lowest lstat is reference category
```
```{r}
table(cut(Boston$lstat, 5))
```

### 8.Optional: Create a piecewise regression model pwq_mod where the sections are not equally spaced, but have equal amounts of training data. Hint: use the quantile() function.

```{r}
breaks<-c(-Inf,quantile(Boston$lstat, probs = c(0.2,0.4,0.6,0.8)),Inf)
pwq_mod<-lm(medv~cut(lstat,breaks =breaks),data=Boston)
pred_plot(pwq_mod)
```
```{r}
table(cut(Boston$lstat, breaks))

```
# Piecewise polynomial regression
### 9. This function does not have comments. Copy - paste the function and add comments to each line. To figure out what each line does, you can first create “fake” vec and knots variables, for example vec <- 1:20 and knots <- 2 and try out the lines separately.
```{r}
#vec is independent variable; knots is number splits we do 
piecewise_cubic_basis <- function(vec, knots = 0) {
  #we want to fit polynomial of 3rd degree to each of spitted parts of data
  if (knots == 0) return(poly(vec, degree = 3, raw = TRUE))
  #we split a vector into knots+1 equally spaced intervals
  cut_vec <- cut(vec, breaks = knots + 1)
  #we initiate a matrix of the lenght of the vector
  out <- matrix(nrow = length(vec), ncol = 0)
  #we loop throught knot+1 splits of the vector 
  for (lvl in levels(cut_vec)) {
    #assign the vector to tmp
    tmp <- vec
    #if the value of the vector is not in current lvl split set it to 0
    tmp[cut_vec != lvl] <- 0
    #add polynomial of 3rd order matrix to the out matrix
    out <- cbind(out, poly(tmp, degree = 3, raw = TRUE))
  }
  #return the piecewise polynomial matrix
  out
}
```
### 10.Create piecewise cubic models with 1, 2, and 3 knots (pc1_mod - pc3_mod) using this piecewise cubic basis function. Compare them using the pred_plot() function.
```{r}
#just to check if it returns the 3rd degree polynomial
pc0_mod<-lm(medv~piecewise_cubic_basis(lstat),data=Boston)
pred_plot(pc0_mod) #it is indeed 

pc1_mod<-lm(medv~piecewise_cubic_basis(lstat,1),data=Boston)
pred_plot(pc1_mod)#this one is already yielding a good prediction I think
pc2_mod<-lm(medv~piecewise_cubic_basis(lstat,2),data=Boston)
pred_plot(pc2_mod)
pc3_mod<-lm(medv~piecewise_cubic_basis(lstat,3),data=Boston)
pred_plot(pc3_mod)
```
# Splines

### 11.Create a data frame called boston_tpb with the columns medv and lstat from the Boston dataset.
```{r}
boston_tpb<-data.frame(medv=Boston$medv,lstat=Boston$lstat)
```

### 12.Now use mutate to add squared and cubed versions of the lstat variable to this dataset.
```{r}
boston_tpb<-boston_tpb %>% mutate(lstat_v2=lstat^2,lstat_v3=lstat^3)
```

### 13.Use mutate to add a column lstat_tpb to this dataset which is 0 below the median and has value (lstat - median(lstat))^3 above the median. Tip: you may want to use ifelse() within your mutate() call.
```{r}
boston_tpb<-boston_tpb %>% mutate(lstat_tpb=if_else(lstat < median(lstat),0,(lstat - median(lstat))^3))
head(boston_tpb)
```
### 14.Create a linear model tpb_mod using the lm() function. How many predictors are in the model? How many degrees of freedom does this model have?
```{r}
tpb_mod<-lm(medv~.,data=boston_tpb)
summary(tpb_mod)
```
This model got 5 predictors and also 5 degrees of freedom 

### 15. Create a cubic spline model bs1_mod with a knot at the median using the bs() function. Compare its predictions to those of the tpb_mod using the predict() function on both models.
```{r}
bs1_mod<-lm(medv~bs(lstat,knots=median(lstat)),data=Boston)
summary(bs1_mod)
```

```{r}
mean(abs(predict(bs1_mod) - predict(tpb_mod)))
```
### 16.Create a prediction plot from the bs1_mod object using the pred_plot() function.
```{r}
pred_plot(bs1_mod)
```

### 17.Create a natural cubic spline model (ns3_mod) with 3 degrees of freedom using the ns() function. Plot it, and compare it to the bs1_mod.
```{r}
ns3_mod<-lm(medv~ns(lstat,df=3),data=Boston)
pred_plot(ns3_mod)
summary(ns3_mod)
```

The line represent a similarly well fit but with linear patterns at the ends. The R^2 is slightly worse for this models but the prediction line seems more sensible. 

### 18.Plot lin_mod, pn3_mod, pw5_mod, pc3_mod, bs1_mod, and ns3_mod and give them nice titles by adding + ggtitle("My title") to the plot. You may use the function plot_grid() from the package cowplot to put your plots in a grid.
```{r}
library(cowplot)
plot_grid(
  pred_plot(lin_mod) + ggtitle("Linear regression"),
  pred_plot(pn3_mod) + ggtitle("Polynomial"),
  pred_plot(pw5_mod) + ggtitle("Piecewise constant"),
  pred_plot(pc3_mod) + ggtitle("Piecewise cubic"),
  pred_plot(bs1_mod) + ggtitle("Cubic spline"),
  pred_plot(ns3_mod) + ggtitle("Natural spline")
)
```

### 19. Use 12-fold cross validation to determine which of the 6 methods (lin, pn3, pw5, pc3, bs1, and ns3) has the lowest out-of-sample MSE.

```{r}
# first create an mse function
mse <- function(y_true, y_pred) mean((y_true - y_pred)^2)

# add a 12 split column to the boston dataset so we can cross-validate
boston_cv <- Boston %>% mutate(split = sample(rep(1:12, length.out = nrow(Boston))))

# prepare an output matrix with 12 slots per method for mse values
output_matrix <- matrix(nrow = 12, ncol = 6) 
colnames(output_matrix) <- c("lin", "pn3", "pw5", "pc3", "bs1", "ns3")

# loop over the splits, run each method, and return the mse values
for (i in 1:12) {
  train <- boston_cv %>% filter(split != i)
  test  <- boston_cv %>% filter(split == i)
  
  brks <- c(-Inf, 7, 15, 22, Inf)
  
  lin_mod <- lm(medv ~ lstat,                            data = train)
  pn3_mod <- lm(medv ~ poly(lstat, 3),                   data = train)
  pw5_mod <- lm(medv ~ cut(lstat, brks),                 data = train)
  pc3_mod <- lm(medv ~ piecewise_cubic_basis(lstat, 3),  data = train)
  bs1_mod <- lm(medv ~ bs(lstat, knots = median(lstat)), data = train)
  ns3_mod <- lm(medv ~ ns(lstat, df = 3),                data = train)
  
  output_matrix[i, ] <- c(
    mse(test$medv, predict(lin_mod, newdata = test)),
    mse(test$medv, predict(pn3_mod, newdata = test)),
    mse(test$medv, predict(pw5_mod, newdata = test)),
    mse(test$medv, predict(pc3_mod, newdata = test)),
    mse(test$medv, predict(bs1_mod, newdata = test)),
    mse(test$medv, predict(ns3_mod, newdata = test))
  )
}

# this is the comparison of the methods
colMeans(output_matrix)
##      lin      pn3      pw5      pc3      bs1      ns3 
## 38.76726 29.36707 37.95038 29.22791 27.51021 28.68858
# we can show it graphically too
tibble(names = as_factor(colnames(output_matrix)), 
       mse   = colMeans(output_matrix)) %>% 
  ggplot(aes(x = names, y = mse, fill = names)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  scale_fill_viridis_d(guide = "none") +
  labs(
    x     = "Method", 
    y     = "Mean squared error", 
    title = "Comparing regression method prediction performance"
  )
```
The last two models fit the data the best !
