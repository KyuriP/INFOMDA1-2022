---
title: "Practical_7"
author: "Hasbini Laura"
date: "19 octobre 2022"
output: html_document
---

# Supervised learning: Nonlinear Regression

```{r}
library(MASS)
library(splines)
library(ISLR)
library(tidyverse)
```

```{r}
set.seed(45)
```

## Prediction plot

```{r}
Boston %>% 
  ggplot(aes(x = lstat, y = medv)) +
  geom_point() +
  theme_minimal()
```

#### Question 1 
"Create a function called pred_plot() that takes as input an lm object, which outputs the above plot but with a prediction line generated from the model object using the predict() method."
```{r}
pred_plot <- function(lm_model){
  x_pred <- seq(from=min(Boston$lstat), to=max(Boston$lstat), length.out = 500)
  y_pred <- predict(lm_model, newdata = tibble(lstat = x_pred))
  pred <- tibble(lstat = x_pred, medv=y_pred)
  
  Boston %>% 
    ggplot(aes(x = lstat, y = medv)) +
    geom_point() +
    geom_line(data=pred, size=1, col="blue") +
    theme_minimal()
}
```

#### Question 2 
"Create a linear regression object called lin_mod which models medv as a function of lstat. Check if your prediction plot works by running pred_plot(lin_mod). Do you see anything out of the ordinary with the predictions?"

```{r}
lin_mod <- lm(medv ~ lstat, data = Boston)

pred_plot(lin_mod)
```

The prediction closzly follows the observations, howeve the linearity of the curve imply negative median housing value for high lstat. 

#Polynomial regression 

#### Question 3 
"Create another linear model pn3_mod, where you add the second and third-degree polynomial terms I(lstat^2) and I(lstat^3) to the formula. Create a pred_plot() with this model."

```{r}
pn3_mod <- lm(medv ~ lstat + I(lstat^2) + I(lstat^3), data = Boston)

pred_plot(pn3_mod)
```

We using higher order polynomial we can see that we are closer to the observations. Also the model is not predicting negative values anymore. 

#### Question 4 
"Play around with the poly() function. What output does it generate with the arguments degree = 3 and raw = TRUE?"

```{r}
poly(1:5, degree = 3, raw=TRUE)
```

#### Question 5 
"Use the poly() function directly in the model formula to create a 3rd-degree polynomial regression predicting medv using lstat. Compare the prediction plot to the previous prediction plot you made. What happens if you change the poly() function to raw = FALSE?
"

```{r}
pn3_mod2 <- lm(medv ~ poly(lstat, degree=3, raw=TRUE), data = Boston)

pred_plot(pn3_mod2)
```

The result obtained when setting 'raw=TRUE' is higly similar to the one obtained previously. 

```{r}
pn3_mod2 <- lm(medv ~ poly(lstat, degree=3, raw=FALSE), data = Boston)

pred_plot(pn3_mod2)
```

When setting 'rax=FALSE', the model will compute and orthogonal polynomial. It doesn't seems to change significantly the performance of the regression. 

## Piecewise regression 

#### Question 6 
"Create a model called pw2_mod with one predictor: I(lstat <= median(lstat)). Create a pred_plot with this model. Use the coefficients in coef(pw2_mod) to find out what the predicted value for a low-lstat neighbourhood is."

```{r}
pw2_mod <- lm(medv ~ I(lstat <= median(lstat)), data = Boston)
pred_plot(pw2_mod)
```

```{r}
coef(pw2_mod)
```

The model above corresponds to a constant piecewise regression. 
For the low-stat neighbourhood, the predicted value is approwimatly $28.39$

#### Question 7 
"Use the cut() function in the formula to generate a piecewise regression model called pw5_mod that contains 5 equally spaced sections. Again, plot the result using pred_plot."

```{r}
pw5_mod <- lm(medv ~ cut(lstat,5), data = Boston)
pred_plot(pw5_mod)
```
```{r}
table(cut(Boston$lstat, 5))
```


When using the function cut we can see that one more time we are constructing a constant piecewise regression, but this time with 5 sections. 

#### Question 8 
"Optional: Create a piecewise regression model pwq_mod where the sections are not equally spaced, but have equal amounts of training data. Hint: use the quantile() function."

```{r}
breaks <- c(-Inf, quantile(Boston$lstat, probs = c(0.2, 0.4, 0.6, 0.8)), Inf)
pwq_mod <- lm(medv ~cut(lstat, breaks), data = Boston)
pred_plot(pwq_mod)
```

The piecewise regression seems to performs better when the lstat is cutted regarding the quantiles 

## Piecewise polynomial regression

#### Question 9 
"This function does not have comments. Copy - paste the function and add comments to each line. To figure out what each line does, you can first create “fake” vec and knots variables, for example vec <- 1:20 and knots <- 2 and try out the lines separately.
"
```{r}
#Creation of a function called 'piecewise_cubis_basis which takes in arguments the vector and the number of regions desired.
piecewise_cubic_basis <- function(vec, knots = 1) {
  #If we don't want any not, a usual regression with a 3th degree polynomial is performed 
  if (knots == 0) return(poly(vec, degree = 3, raw = TRUE))
  
  #Division of the initial vector in K regions
  cut_vec <- cut(vec, breaks = knots + 1)
  
  #Creation of a matrxi which has no colum the the same number of lines as the original vector
  out <- matrix(nrow = length(vec), ncol = 0)
  
  #Loop over the K regions of cut_vec
  for (lvl in levels(cut_vec)) {
    tmp <- vec # Copy the original vector in a temporary variable
    tmp[cut_vec != lvl] <- 0 #Assign 0 to all the points out of the region 
    out <- cbind(out, poly(tmp, degree = 3, raw = TRUE)) #Save in the matrix out the polynomial regression done on this section 
  }
  
  out
}
```

#### Question 10
"Create piecewise cubic models with 1, 2, and 3 knots (pc1_mod - pc3_mod) using this piecewise cubic basis function. Compare them using the pred_plot() function."

```{r}
pc1_mod <- lm(medv ~ piecewise_cubic_basis(lstat, 1), data=Boston)
pred_plot(pc1_mod)
```

```{r}
pc2_mod <- lm(medv ~ piecewise_cubic_basis(lstat, 2), data=Boston)
pred_plot(pc2_mod)
```

```{r}
pc3_mod <- lm(medv ~ piecewise_cubic_basis(lstat, 3), data=Boston)
pred_plot(pc3_mod)
```

When increasing the number of note it seems that the prediction is closer to the original data on each section. However, as we didn't add any constraint on continuity, $1^{st}$ and $2^{nd}$ order derivative, we can also see some jumps between the sections. Overall we can thus deduce that the best options are either to constrain continuity or to choose only 1/2 section. 

## Splines 

#### Question 11
"Create a data frame called boston_tpb with the columns medv and lstat from the Boston dataset."

```{r}
boston_tpb <- tibble(medv = Boston$medv, lstat = Boston$lstat)
```

#### Question 12 
"Now use mutate to add squared and cubed versions of the lstat variable to this dataset."

```{r}
boston_tpb <- boston_tpb %>% mutate(lstat2 = I(lstat^2), 
                      lstat3 = I(lstat^3))
```


#### Question 13
"Use mutate to add a column lstat_tpb to this dataset which is 0 below the median and has value (lstat - median(lstat))^3 above the median. Tip: you may want to use ifelse() within your mutate() call."

```{r}
boston_tpb <- boston_tpb %>% mutate(lstat_tpb = ifelse(lstat<median(lstat), 0, (lstat - median(lstat))^3))
```

#### Question 14 
"Create a linear model tpb_mod using the lm() function. How many predictors are in the model? How many degrees of freedom does this model have?"

```{r}
tpb_mod <- lm(medv ~ ., data = boston_tpb)
```

This model has 5 predictors : Intercept, lstat, lstat^2, lstat^3 and lstat_tpb and 5 degree of freedom. 

#### Question 15 
"Create a cubic spline model bs1_mod with a knot at the median using the bs() function. Compare its predictions to those of the tpb_mod using the predict() function on both models."

```{r}
bs1_mod <- lm(medv ~ bs(lstat, knots = median(lstat)), data = Boston)
```

```{r}
mean(abs(predict(bs1_mod)-predict(tpb_mod)))
```

The difference between the two models is extremely small, thus suggesting that they have similar performances. 

#### Question 16 
"Create a prediction plot from the bs1_mod object using the plot_pred() function."

```{r}
pred_plot(bs1_mod)
```
The models seems to performs really well except for high lstat where the line slopes up

#### Question 17
"Create a natural cubic spline model (ns3_mod) with 3 degrees of freedom using the ns() function. Plot it, and compare it to the bs1_mod."

```{r}
ns3_mod <- lm(medv ~ns(lstat, df=3), data = Boston)
pred_plot(ns3_mod)
```

The fit seems to performs overall less well than bs1. However, it's physically more accurate as the curve is not increasing again for high lstat. 

#### Question 18 
"Plot lin_mod, pn3_mod, pw5_mod, pc3_mod, bs1_mod, and ns3_mod and give them nice titles by adding + ggtitle("My title") to the plot. You may use the function plot_grid() from the package cowplot to put your plots in a grid."

```{r}
lin_plot <- pred_plot(lin_mod) + ggtitle("Linear")
pn2_plot <- pred_plot(pn3_mod) + ggtitle("Polynomial")
pw5_plot <- pred_plot(pw5_mod) + ggtitle("Piecewise constant")
pc2_plot <- pred_plot(pc2_mod) + ggtitle("Piecewise cubic")
bs1_plot <- pred_plot(bs1_mod) + ggtitle("Cubic spline")
ns3_plot <- pred_plot(ns3_mod) + ggtitle("Natural spline")

cowplot::plot_grid(lin_plot, pn2_plot, pw5_plot, pc2_plot, bs1_plot, ns3_plot)
```

## Programming assignment 

#### Question 19 
"Use 12-fold cross validation to determine which of the 6 methods (lin, pn3, pw5, pc3, bs1, and ns3) has the lowest out-of-sample MSE."

```{r}
mse <- function(y_true, y_pred){mean((y_true-y_pred)^2)}

boston_cv <- Boston %>% mutate(split = sample(rep(1:12, length.out=length(Boston$lstat))))

#The following matrix will return the score obtained for each meathod on each K validation set. 
output_matrix <- matrix(nrow=12, ncol=6)
colnames(output_matrix) <- c("lin", "pn3", "pw5", "pc3", "bs1", "ns3")

#Loop to compute the score 
for(i in 1:12){
  train <- boston_cv %>% filter(split !=i)
  test <- boston_cv %>% filter(split == i)
  
  breaks <- c(-Inf, 7, 15, 22, Inf)
  
  lin_mod <- lm(medv ~lstat, data = train)
  pn3_mod <- lm(medv ~poly(lstat,3), data = train)
  pw5_mod <- lm(medv ~cut(lstat,breaks), data = train)
  pc3_mod <- lm(medv ~piecewise_cubic_basis(lstat,3), data = train)
  bs1_mod <- lm(medv ~bs(lstat, knots=median(lstat)), data = train)
  ns3_mod <- lm(medv ~ns(lstat, df=3), data = train)
  
  scores = c(mse(test$medv, predict(lin_mod, newdata = test)), 
            mse(test$medv, predict(pn3_mod, newdata = test)), 
            mse(test$medv, predict(pw5_mod, newdata = test)), 
            mse(test$medv, predict(pc3_mod, newdata = test)), 
            mse(test$medv, predict(bs1_mod, newdata = test)), 
            mse(test$medv, predict(ns3_mod, newdata = test)))
  output_matrix[i, ] <- scores
  
}

colMeans(output_matrix)
```

The best method, seems to be the pn3 or in other word the polynomial method. The cubic spline also performs relatively well but presents an unphysical increase for high lstat values 





