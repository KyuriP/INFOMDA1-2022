---
title: "P8_ Ensemble Methods"
author: "Christoph Völtzke"
date: "2022-11-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
```{r}
library(tidyverse)
library(magrittr)
library(psych)
library(caret)
library(gbm)
library(xgboost)
library(data.table)
library(ggforce)
```

```{r}
set.seed(45)
df <- readRDS("data/train_disease.RDS")
```

## 1. Get an impression of the data by looking at the structure of the data and creating some descriptive statistics.
```{r}
head(df)
tail(df)

df %>%
  select(-c(Gender, Disease)) %>%
  describeBy(df$Disease, fast = TRUE)
```

## 2. To further explore the data we work with, create some interesting data visualizations that show whether there are interesting patterns in the data.

```{r}
df %>%
  select(-Gender) %>%
  pivot_longer(where(is.numeric)) %>%
  ggplot(aes(x = value, col = Disease, fill = Disease)) +
  geom_boxplot(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()
```

## 3. Shortly reflect on the difference between bagging, random forests, and boosting.
 Bagging:\
 fit a regression tree to N bootstrap samples of the training data take the average of all classification trees to base predictions on\
 Random forest:\
 Similarly to bagging, classification trees are trained on a bootstrap sample of the data. However, the decision trees are trained using less than all features in the data. \
Boosting:     \
We build a decision tree sequentially. Given the currentwe fit a (small) tree on the residuals of the current model, rather than on the outcome Y

## 4. Apply bagging to the training data, to predict the outcome Disease, using the caret library.
```{r}
cvcontrol <- trainControl(method = "repeatedcv", 
                          number = 10,
                          allowParallel = TRUE)
```

```{r}
bag_train <- train(Disease ~ .,
                   data = df, 
                   method = 'treebag',
                   trControl = cvcontrol,
                   importance = TRUE)
```

## 5. Interpret the variable importance measure using the varImp function on the trained model object.
```{r}
bag_train %>%
  varImp %>%
  plot
```

## 6. Create training set predictions based on the bagged model, and use the confusionMatrix() function from the caret package to assess it’s performance.`

```{r}
confusionMatrix(predict(bag_train, type = "raw"),
                df$Disease)
bag_train # because here we do cross validation with the other prediction it is just perfect seperation
```

## 8. Fit a random forest to the training data to predict the outcome Disease, using the caret library.
```{r}
rf_train <- train(Disease ~ .,
                  data = df, 
                  method = 'rf',
                  trControl = cvcontrol,
                  importance = TRUE)
```

## 9. Again, interpret the variable importance measure using the varImp function on the trained model object. Do you draw the same conclusions as under the bagged model?
```{r}
rf_train %>%
  varImp %>%
  plot
## The random forest model indicates that other variables are more important,
## as compared to the bagged model.
```

## 10. Output the model output from the random forest. Are we doing better than with the bagged model?
```{r}
rf_train
```

## 11. Now, fit a boosting model using the caret library to predict disease status.`
```{r}
gbm_train <- train(Disease ~ .,
                   data = df,
                   method = "gbm",
                   verbose = F,
                   trControl = cvcontrol)
```

## 12. Again, interpret the variable importance measure. You will have to call for summary() on the model object you just created. Compare the output to the previously obtained variable importance measures.
```{r}
summary(gbm_train)
```
## 13. Output the model output from our gradient boosting procedure. Are we doing better than with the bagged and random forest model?

```{r}
gbm_train
# Yes, our best model is doing slightly better then the previous two models.
# However, this might still be random variation.
```
## 14. Download the file shap.R from this Github repository.

```{r}
library(devtools)
source_url("https://github.com/pablo14/shap-values/blob/master/shap.R?raw=TRUE")
```

## 15. Specify your model as follows, and use it to create predictions on the training data.
```{r}
train_x <- model.matrix(Disease ~ ., df)[,-1]
train_y <- as.numeric(df$Disease) - 1
xgboost_train <- xgboost(data = train_x,
                         label = train_y, 
                         max.depth = 10,
                         eta = 1,
                         nthread = 4,
                         nrounds = 4,
                         objective = "binary:logistic",
                         verbose = 2)



pred <- tibble(Disease = predict(xgboost_train, newdata = train_x)) %>%
  mutate(Disease = factor(ifelse(Disease < 0.5, 1, 2),
                          labels = c("Healthy", "Disease")))

table(pred$Disease, df$Disease)
```

## 16. First, calculate the SHAP rank scores for all variables in the data, and create a variable importance plot using these values. Interpret the plot.
```{r}
shap_results <- shap.score.rank(xgboost_train,
                                X_train = train_x,
                                shap_approx = F)
var_importance(shap_results)
```

## 17. Plot the SHAP values for every individual for every feature and interpret them.
```{r}
shap_long <- shap.prep(shap = shap_results,
                       X_train = train_x)

plot.shap.summary(shap_long)
xgb.plot.shap(train_x, features = colnames(train_x), model = xgboost_train, n_col = 3)
## The first plot shows, for example, that those with a high value for 
## Direct_Bilirubin have a lower probability of being diseased. Also,
## Those with a higher age have a lower probability of being diseased,
## while those with a higher Albumin have a higher probability of being diseased.

## The second set of plots displays the marginal relationships of the SHAP values with the predictors. This conveys the same information, but in greater detail. The interpretability may be a bit tricky for the inexperienced data analyst. 
```
## 18. Verify which of the models you created in this practical performs best on the test data.
```{r}
test <- readRDS("data/test_disease.RDS")

bag_test <- predict(bag_train, newdata = test)
rf_test  <- predict(rf_train, newdata = test)
gbm_test <- predict(gbm_train, newdata = test)
xgb_test <- predict(xgboost_train, newdata = model.matrix(Disease ~ ., test)[,-1]) %>%
  factor(x = ifelse(. < 0.5, 1, 2), levels = c(1,2), labels = c("Healthy", "Disease"))

list(bag_test, 
     rf_test, 
     gbm_test, 
     xgb_test) %>%
  map(~ confusionMatrix(.x, test$Disease))
```







