---
title: "Ensemble methods"
author: "Ana Martins"
date: "November 2022"
output: html_document
---

```{r}
library(tidyverse)
library(magrittr)
library(psych)
library(caret)
library(gbm)
library(xgboost)
library(data.table)
library(ggforce)
```

```{r}
set.seed(45)
df <-
  read_csv("data/Indian_Liver_Patient_Dataset_(ILPD).csv", col_names = FALSE) %>%
  mutate(
    age = X1,
    gender = X2,
    tb = X3,
    db = X4,
    alkphos = X5,
    sgpt = X6,
    sgot = X7,
    tp = X8,
    alb = X9,
    ag = X10,
    split = X11
  ) %>%
  select(-X1,-X2,-X3,-X4,-X5,-X6,-X7,-X8,-X9,-X10,-X11)
```

```{r}
summary(df)

train_df <- df %>%
  filter(split == 1) %>%
  select(-split)
test_df <- df %>%
  filter(split == 2) %>%
  select(-split)
```

1. **Get an impression of the data by looking at the structure of the data and creating some descriptive statistics.**

2. **2. To further explore the data we work with, create some interesting data visualizations that show whether there are interesting patterns in the data.**

```{r}
train_df %>% 
  ggplot(aes(x = age)) +
  geom_point(aes(y = tb), color = "red") +
  geom_point(aes(y = db), color = "orange") +
  geom_point(aes(y = ag), color = "yellow") +
  geom_point(aes(y = sgpt), color = "green") +
  geom_point(aes(y = sgot), color = "cyan") +
  geom_point(aes(y = tp), color = "blue") +
  geom_point(aes(y = alb), color = "purple") +
  geom_point(aes(y = ag), color = "brown")
```

This is not very useful. Cyan (`sgot`) and green (`sgpt`) seem to be much larger than the others.

```{r}
train_df %>% 
  ggplot(aes(x = age)) +
  geom_point(aes(y = sgpt), color = "green") +
  geom_point(aes(y = sgot), color = "cyan")
```

Not very useful either.

```{r}
train_df %>% 
  ggplot(aes(y = tb)) +
  geom_boxplot()

train_df %>% 
  ggplot(aes(y = db)) +
  geom_boxplot()

train_df %>% 
  ggplot(aes(y = alkphos)) +
  geom_boxplot()

train_df %>% 
  ggplot(aes(y = sgpt)) +
  geom_boxplot()
train_df %>% 
  ggplot(aes(y = sgot)) +
  geom_boxplot()

train_df %>% 
  ggplot(aes(y = tp)) +
  geom_boxplot()

train_df %>% 
  ggplot(aes(y = alb)) +
  geom_boxplot()

train_df %>% 
  ggplot(aes(y = ag)) +
  geom_boxplot()
```

Most of the variables have multiple outliers, which we can assume would be the non-patients, since we have more patients than non-patients.

There is no `Disease` variable though...

3. **Shortly reflect on the difference between bagging, random forests, and boosting.**

**Bagging** is a general-purpose procedure for reducing the variance of a statistical learning method. It takes repeated samples from the dataset, builds a separate prediction model for each sample and averages the resulting predictions.

**Random forests** provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of *m* predictors is chosen as split candidates from the full set of *p* predictors. The split is allowed to use only one of those *m* predictors. A fresh sample of *m* predictors is taken at each split, and typically we choose *m ≈ √ p* —that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors. In other words, in building a random forest, at each split in the tree,
the algorithm is not even allowed to consider a majority of the available predictors.

**Boosting** works similarly to bagging, except that the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.

14. **14. Download the file `shap.R` from [this](https://github.com/pablo14/shap-values) Github repository.**

```{r}
library(devtools)
source_url("https://github.com/pablo14/shap-values/blob/master/shap.R?raw=TRUE")
```

