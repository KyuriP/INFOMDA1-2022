---
title: "Ensemble methods"
author: "Ana Martins"
date: "November 2022"
output: html_document
---

```{r}
library(tidyverse)
library(magrittr)
library(psych)
library(caret)
library(gbm)
library(xgboost)
library(data.table)
library(ggforce)
```

```{r}
set.seed(45)
df <- readRDS("data/train_disease.RDS")
```

1. **Get an impression of the data by looking at the structure of the data and creating some descriptive statistics.**

```{r}
summary(df)
```


2. **2. To further explore the data we work with, create some interesting data visualizations that show whether there are interesting patterns in the data.**

```{r}
df %>% 
  ggplot(aes(x = Disease, y = Total_Bilirubin)) +
  geom_point()
```


3. **Shortly reflect on the difference between bagging, random forests, and boosting.**

Bagging is a general-purpose procedure for reducing the variance of a statistical learning method. It takes repeated samples from the dataset, builds a separate prediction model for each sample and averages the resulting predictions.

Random forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of *m* predictors is chosen as split candidates from the full set of *p* predictors. The split is allowed to use only one of those *m* predictors. A fresh sample of *m* predictors is taken at each split, and typically we choose *m ≈ √ p* —that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors. In other words, in building a random forest, at each split in the tree,
the algorithm is not even allowed to consider a majority of the available predictors.