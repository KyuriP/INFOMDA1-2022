---
title: "P5 A Dacko"
author: "Aleksandra Dacko"
date: "10/10/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(class)
library(ISLR)
library(tidyverse)
library(ggplot2)
library(magrittr)
```

```{r}
set.seed(45)
```
# Default dataset

### 1. Create a scatterplot of the Default dataset, where balance is mapped to the x position, income is mapped to the y position, and default is mapped to the colour. Can you see any interesting patterns already?
```{r}
head(Default)
```
```{r}
Default %>% ggplot(aes(x=balance,y=income,color=default)) + geom_point()
```

### 2.Add facet_grid(cols = vars(student)) to the plot. What do you see?
```{r}
Default %>% ggplot(aes(x=balance,y=income,color=default)) + geom_point()+facet_grid(cols=vars(Default$student))
```

### 3. Transform “student” into a dummy variable using ifelse() (0 = not a student, 1 = student). Then, randomly split the Default dataset into a training set default_train (80%) and a test set default_test (20%)

```{r}
Default %<>% mutate(student=case_when(student=="No" ~0, student=="Yes"~1))
```

```{r}
nobs <- nrow(Default) # Number of rows
idx_df <- 1:nobs # Indices of rows
Default<- Default[sample(idx_df), ] # Randomize
# Split the data into 80% train and 20% validation data
train_idx <- seq(1, nobs * 0.8) # Training data indices
default_train<-Default[train_idx,]
default_test<-Default[-train_idx,] 
```


### 4.Create class predictions for the test set using the knn() function. Use student, balance, and income (but no basis functions of those variables) in the default_train dataset. Set k to 5. Store the predictions in a variable called knn_5_pred

```{r}
knn_5_pred <-knn(train=default_train[,2:4],test=default_test[,2:4], k=5,cl=as.factor(default_train[,1]))
```

### 5. Create two scatter plots with income and balance as in the first plot you made. One with the true class (default) mapped to the colour aesthetic, and one with the predicted class (knn_5_pred) mapped to the colour aesthetic.
```{r}
default_test %>%ggplot(aes(x=balance,y=income,color=default)) + geom_point()
```

```{r}
bind_cols(default_test, pred = knn_5_pred) %>% arrange(default) %>% ggplot(aes(x=balance,y=income,color=pred)) + geom_point()
```

```{r}
table(default_test$default, pred = knn_5_pred)
```
### 6.Repeat the same steps, but now with a knn_2_pred vector generated from a 2-nearest neighbours algorithm. Are there any differences?
```{r}
knn_2_pred <-knn(train=default_train[,2:4],test=default_test[,2:4], k=2, cl=as.factor(default_train[,1]))
```


```{r}
bind_cols(default_test, pred = knn_2_pred) %>% arrange(default) %>% ggplot(aes(x=balance,y=income,color=pred)) + geom_point()
```
# Confusion matrix
```{r}
table(true=default_test$default, predicted = knn_2_pred)
```
5-KKN seems to perform a bit better 

### 7.What would this confusion matrix look like if the classification were perfect?
```{r}
table(true=default_test$default, predicted = default_test$default)

```
### 8.Make a confusion matrix for the 5-nn model and compare it to that of the 2-nn model. What do you conclude?
```{r}
#5-nn model
table(default_test$default, pred = knn_5_pred)
#2-nn model
table(true=default_test$default, predicted = knn_2_pred)
```
# Logistic regression
In terms of the prue positives and false negatives the performance of 5-nn model is better.
### 9.Use glm() with argument family = binomial to fit a logistic regression model lr_mod to the default_train data.
```{r}
lr_model<-glm(default_train, formula =default~student+balance+income, family = binomial )
prediction<-predict(lr_model,type="response")
```
### 10. Visualise the predicted probabilities versus observed class for the training dataset in lr_mod. You can choose for yourself which type of visualisation you would like to make. Write down your interpretations along with your plot.

```{r}
bind_cols(def=default_train$default, pred = prediction)  %>% ggplot(aes(x=def,y=pred, color=def)) + geom_point(position = position_jitter(width = 0.2), alpha = .3)
```


### 11.Look at the coefficients of the lr_mod model and interpret the coefficient for balance. What would the probability of default be for a person who is not a student, has an income of 40000, and a balance of 3000 dollars at the end of each month? Is this what you expect based on the plots we’ve made before?
```{r}
summary(lr_model)
```
```{r}

#now we calculate the log odds 
lo<-as.numeric(coefficients(lr_model)[1]+0*coefficients(lr_model)[2]+3000+coefficients(lr_model)[3]+ 4000*coef(lr_model)[4])
p<-round(1/(1+exp(-lo)),10)
p
```
So the probability of defaulting is 1. We could expect that happening 


#Visualising the effect of the balance variable
### 12.Create a data frame called balance_df with 3 columns and 500 rows: student always 0, balance ranging from 0 to 3000, and income always the mean income in the default_train dataset.

```{r}
balance_df<-data.frame(student=rep(0,500),balance=seq(from=1,to=3000,length.out=500),income=rep(mean(default_train$income),500))

```

### 13.Use this dataset as the newdata in a predict() call using lr_mod to output the predicted probabilities for different values of balance. Then create a plot with the balance_df$balance variable mapped to x and the predicted probabilities mapped to y. Is this in line with what you expect?
```{r}
predicted<-predict(lr_model,newdata = balance_df,type="response")

bind_cols(balance=balance_df$balance,pred=predicted) %>% ggplot(aes(x=balance,y=pred))+geom_line(col="pink",size=2)+theme_minimal()
```


### 14. Create a confusion matrix just as the one for the KNN models by using a cutoff predicted probability of 0.5. Does logistic regression perform better?
```{r}
pred<-predict(lr_model,newdata =default_test,type="response" )
pred_lr<-factor(pred>0.5, labels = c("No","Yes"))
table(predicted=pred_lr,true=default_test$default)
```
In general the logistic regression perform indeed better than the knn approach 

# Linear discriminant analysis

### 15.Train an LDA classifier lda_mod on the training set. &  16. Look at the lda_mod object. What can you conclude about the characteristics of the people who default on their loans?
```{r}
lda_mod<-lda(data=default_train,default~. )
lda_mod
```

The defaulters got based on my training data larger proportion of people being students, more than double as large mean balance and slightly lower income in comparison to non-defaulters. 
### 17.Create a confusion matrix and compare it to the previous methods.
```{r}
pred<-predict(lda_mod,newdata = default_test)
table(predicted=pred$class,true=default_test$default)
```
I would say it is similar to the logistic regression 
# Final assignment

### 18. Create a model (using knn, logistic regression, or LDA) to predict whether a 14 year old boy from the 3rd class would have survived the Titanic disaster. You can find the data in the data/ folder. Would the passenger have survived if they were a girl in 2nd class?
```{r}
titanic<-read.csv(file = "data/titanic.csv",header = T)
```

```{r}
#check and clean data. Transform ordinary and binary variables to factors. Make female the reference category
head(titanic)
data<-titanic %>% select(!Name) %>%  mutate_if(is.character, as.factor)
head(data)
```

```{r}
#now I use logistic regression to predict the probability of survival
model_glm<-glm(Survived~PClass*Age*Sex,data=data,family = "binomial")

probability<- function(model,PClass,Age,Sex){
  new<-tibble(PClass=PClass,
              Age=Age,
              Sex=Sex)
  p<-predict(model,newdata=new,type="response")
  return(p)
}
#boy
probability(model = model_glm,PClass ="3rd",Age =14 ,Sex ="male" )
#girl
probability(model = model_glm,PClass ="2nd" ,Age =14 ,Sex ="female" )

```

