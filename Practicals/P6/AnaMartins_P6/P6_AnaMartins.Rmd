---
title: "Classification evaluation"
author: "Ana Martins"
date: "October 2022"
output: html_document
---

## Introduction

```{r}
library(MASS)
library(ISLR)
library(tidyverse)

library(pROC)

library(rpart)
library(rpart.plot)
library(randomForest)
```

```{r}
set.seed(45)
```


## Confusion matrix, continued

1. **Create a logistic regression model `lr_mod` for this data using the formula `response ~ .` and create a confusion matrix based on a .5 cutoff probability.**

```{r}
data <- read_csv("data/cardiovascular_treatment.csv")
```


```{r}
lr_mod <- glm(formula = "response ~ .", data = data)
```

```{r}
lr_pred <- predict(lr_mod, type = "response")
```

```{r}
lr_pred <- ifelse(lr_pred < 0.5, 0, 1)
```

```{r}
table(true = data$response, pred = lr_pred)
```

### Confusion matrix metrics

2. **Calculate the accuracy, true positive rate (sensitivity), the true negative rate (specificity), the false positive rate, the positive predictive value, and the negative predictive value. You can use the [confusion matrix table on wikipedia](https://en.wikipedia.org/w/index.php?title=Sensitivity_and_specificity&oldid=862159646#Confusion_matrix). What can you say about the model performance? Which metrics are most relevant if this model were to be used in the real world?**

```{r}
tpr <- 91 / (35 + 91) # true positives divided by the positives
tnr <- 78 / (78 + 49) # true negatives divided by the negatives
fpr <- 49 / (78 + 49) # false positives divided by the negatives
ppv <-
  91 / (49 + 91) # true positives divided by the predicted positives
npv <-
  78 / (78 + 35) # true negatives divided by the predicted negatives
```

The model does not seem to be very good. Taking into account we are trying to predict a disease, the most relevant values would be the TPR and the PPV.

3. **Create an LDA model `lda_mod` for the same prediction problem. Compare its performance to the LR model.**

```{r}
lda_mod <- lda(response ~ ., data)
```

```{r}
lda_pred <- predict(lda_mod)
```

```{r}
table(true = data$response, pred = lda_pred$class)
```

It looks the exact same?

4. **Compare the classification performance of `lr_mod` and `lda_mod` for the new patients in the `data/new_patients.csv`.**

```{r}
newdata <- read_csv("data/new_patients.csv")
```


```{r}
lr_pred_new <- predict(lr_mod, newdata = newdata)
```

```{r}
lr_pred_new <- ifelse(lr_pred_new < 0.5, 0, 1)
```

```{r}
table(true = newdata$response, pred = lr_pred_new)
lda_pred_new <- predict(lda_mod, newdata = newdata)
```

```{r}
table(true = newdata$response, pred = lda_pred_new$class)
```

Still the exact same...

### Brier score

**Calculate the out-of-sample brier score for the `lr_mod` and give an interpretation of this number.**

```{r}
lr_pred_percentages <- predict(lr_mod, type = "response")
```

```{r}
lr_sq <- (lr_pred_percentages - data$response) ** 2
BS <- 1 / nrow(data) * sum(lr_sq)
BS
```

The Brier score gives a reasonable result.

### ROC curve

5. **Create two LR models: `lr1_mod` with `severity`, `age`, and `bb_score` as predictors, and `lr2_mod` with the formula `response ~ age + I(age^2) + gender + bb_score * prior_cvd * dose`. Save the predicted probabilities on the training data.**

```{r}
lr1_mod <- glm(response ~ severity + age + bb_score, data = data)
lr2_mod <-
  glm(response ~ age + I(age ^ 2) + gender + bb_score * prior_cvd * dose, data = data)
```

6. **Use the function `roc()` from the `pROC` package to create two ROC objects with the predicted probabilities: `roc_lr1` and `roc_lr2`. Use the `ggroc()` method on these objects to create an ROC curve plot for each. Which model performs better? Why?**

```{r}
lr1_pred <- predict(lr1_mod, type = "response")
```

```{r}
lr1_pred <- ifelse(lr1_pred < 0.5, 0, 1)
```

```{r}
roc_lr1 <- roc(data$response, lr1_pred)
ggroc(roc_lr1)
lr2_pred <- predict(lr2_mod, type = "response")
```

```{r}
lr2_pred <- ifelse(lr2_pred < 0.5, 0, 1)
```

```{r}
roc_lr2 <- roc(data$response, lr2_pred)
ggroc(roc_lr2)
```

The second model performs better, because it has a higher AUC.

7. **Print the `roc_lr1` and `roc_lr2` objects. Which AUC value is higher? How does this relate to the plots you made before? What is the minimum AUC value and what would a “perfect” AUC value be and how would it look in a plot?**

```{r}
roc_lr1
roc_lr2
```

The AUC for the 2nd model. It is the integral of the previous plots. The minimum AUC is 0.5 and the perfect one is 1 and it would look like the left and up sides of a square in a plot.

## Iris dataset

```{r}
# fit lda model, i.e. calculate model parameters
lda_iris <- lda(Species ~ ., data = iris)

# use those parameters to compute the first linear discriminant
first_ld <- -c(as.matrix(iris[,-5]) %*% lda_iris$scaling[, 1])

# plot
tibble(ld = first_ld,
       Species = iris$Species) %>%
  ggplot(aes(x = ld, fill = Species)) +
  geom_histogram(binwidth = .5,
                 position = "identity",
                 alpha = .9) +
  scale_fill_viridis_d(guide =) +
  theme_minimal() +
  labs(x = "Discriminant function",
       y = "Frequency",
       main = "Fisher's linear discriminant function on Iris species") +
  theme(legend.position = "top")
```

8. **Explore the iris dataset using summaries and plots.**

```{r}
summary(iris)
```

```{r}
iris %>% ggplot() + geom_point(mapping = aes(x = Sepal.Length, y = Sepal.Width),
                               color = "blue") + geom_point(mapping = aes(x = Petal.Length, y = Petal.Width),
                                                            color = "green") + facet_wrap(Species ~ .) + ylab("Width") + xlab("Length")
```


## Classification trees

## Final assignment: Random forest for classification