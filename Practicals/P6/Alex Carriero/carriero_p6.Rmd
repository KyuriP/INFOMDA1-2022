---
title: 'SLV: Practical 6'
author: "Alex Carriero"
date: '2022-10-17'
output: html_document
---

# Set up 
```{r, warning = F, message = F}
library(MASS)
library(ISLR)
library(tidyverse)

library(pROC)

library(rpart)
library(rpart.plot)
library(randomForest)
```

```{r}
set.seed(45)
```

### 1. Create a logistic regression model lr_mod for this data using the formula response ~ . and create a confusion matrix based on a .5 cutoff probability.
```{r, warning = F, message = F}
treat <- read_csv("cardiovascular_treatment.csv") %>% 
  mutate(seversity = as.factor(severity), 
         gender    = as.factor(gender), 
         dose      = as.factor(dose), 
         response  = as.factor(response))
```

```{r}
# generate model
lr_mod <- glm(response ~., "binomial", treat)

# impose threshold on predictions
pred_probs <- predict(lr_mod, type = "response")
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

# confusion matrix at 0.5 threshold 
table(true = treat$response, pred = pred_class)
```

# Confusion Matrix Metrics

### 2. Calculate the accuracy, true positive rate (sensitivity), the true negative rate (specificity), the false positive rate, the positive predictive value, and the negative predictive value. You can use the confusion matrix table on wikipedia. What can you say about the model performance? Which metrics are most relevant if this model were to be used in the real world?
```{r}
cmat_lr <- table(true = treat$response, pred = pred_class)

TN <- cmat_lr[1,1]
TP <- cmat_lr[2,2]
FN <- cmat_lr[2,1]
FP <- cmat_lr[1,2]

tibble(
  acc = (TN + TP) / sum(cmat_lr),
  sen = TP / (TP + FN),             # given disease, prob of detecting it
  spc = TN / (TN + FP),             # given no disease, prob of test neg
  fpr = FP / (TN + FP),             # given no disease, prob of false positive
  ppv = TP / (TP + FP),             # given positive test, prob of disease 
  npv = TN / (TN + FN)              # given negative test, prob of no disease
)

# PPV and NPV seem most relevant to me 
# I think it makes sense to determine the probability that the test is accurate 
# conditional on the prediction as in reality we won't know the truth. 
```

### 3. Create an LDA model lda_mod for the same prediction problem. Compare its performance to the LR model.
```{r, warning = F}
lda_mod <- lda(response ~., treat)

pred_lda <- predict(lda_mod)$class
cmat_lda <- table(true = treat$response, pred = pred_lda)

TN <- cmat_lr[1,1]
TP <- cmat_lr[2,2]
FN <- cmat_lr[2,1]
FP <- cmat_lr[1,2]

tibble(
  acc = (TN + TP) / sum(cmat_lda),
  sen = TP / (TP + FN),             # given disease, prob of detecting it
  spc = TN / (TN + FP),             # given no disease, prob of test neg
  fpr = FP / (TN + FP),             # given no disease, prob of false positive
  ppv = TP / (TP + FP),             # given positive test, prob of disease 
  npv = TN / (TN + FN)              # given negative test, prob of no disease
)

# performance is the same 
```

## 4. Compare the classification performance of lr_mod and lda_mod for the new patients in the data/new_patients.csv.
```{r, message = F}
new_data <- read_csv("new_patients.csv") %>%
    mutate(seversity = as.factor(severity), 
         gender    = as.factor(gender), 
         dose      = as.factor(dose), 
         response  = as.factor(response))
```

```{r, warning = F}
# make predictions
pred_probs   <- predict(lr_mod, newdata = new_data, type = "response")
pred_new_lr  <- ifelse(pred_probs > 0.5, 1, 0)
pred_new_lda <- predict(lda_mod, newdata = new_data)$class


# new contingency tables 
cmat_lr  <- table(true = new_data$response, pred = pred_new_lr)
cmat_lda <- table(true = new_data$response, pred = pred_new_lda)

cmat_lr
cmat_lda
```


# Brier Score

Calculate the out-of-sample brier score for the lr_mod and give an interpretation of this number.
```{r}
mean((pred_probs - (as.numeric(new_data$response)-1))^2)

# mse between the probability and the true class is .23
```

# ROC curve 

### 5. Create two LR models: lr1_mod with severity, age, and bb_score as predictors, and lr2_mod with the formula response ~ age + I(age^2) + gender + bb_score * prior_cvd * dose. Save the predicted probabilities on the training data.
```{r}
lr1_mod <- glm( response~ severity + age + bb_score, 
                family = "binomial", data = treat)
lr2_mod <- glm( response~ age + I(age^2) + gender + bb_score * prior_cvd * dose, 
                family = "binomial", data = treat)

lr1_prob <- predict(lr1_mod, type = "response")
lr2_prob <- predict(lr2_mod, type = "response")
```

### 6. Use the function roc() from the pROC package to create two ROC objects with the predicted probabilities: roc_lr1 and roc_lr2. Use the ggroc() method on these objects to create an ROC curve plot for each. Which model performs better? Why?
```{r}
roc_lr1 <- roc(treat$response, lr1_prob)
roc_lr2 <- roc(treat$response, lr2_prob)

ggroc(roc_lr1) + theme_minimal() + labs(title = "LR1")
ggroc(roc_lr2) + theme_minimal() + labs(title = "LR2")

# lr2 is better -- at nearly every cutoff threshold the value of sen and spc are higher 
# looks like there is more area under the lr2 curve
```

### 7.Print the roc_lr1 and roc_lr2 objects. Which AUC value is higher? How does this relate to the plots you made before? What is the minimum AUC value and what would a “perfect” AUC value be and how would it look in a plot?

```{r}
roc_lr1$auc
roc_lr2$auc # higher area 

# perfect AUC is 1, indicating that sensitivity and specificity are both 100% for every 
# in other words: the model can discriminate perfectly 
```

# Iris Dataset 
```{r}
# fit lda model, i.e. calculate model parameters
lda_iris <- lda(Species ~ ., data = iris)

# use those parameters to compute the first linear discriminant
first_ld <- -c(as.matrix(iris[, -5]) %*% lda_iris$scaling[,1])

# plot
tibble(
  ld = first_ld,
  Species = iris$Species
) %>% 
  ggplot(aes(x = ld, fill = Species)) +
  geom_histogram(binwidth = .5, position = "identity", alpha = .9) +
  scale_fill_viridis_d(guide = ) +
  theme_minimal() +
  labs(
    x = "Discriminant function",
    y = "Frequency", 
    main = "Fisher's linear discriminant function on Iris species"
  ) + 
  theme(legend.position = "top")
```

### 8. Explore the iris dataset using summaries and plots.
```{r}
summary(iris)
```

```{r}
# Some example plots you could make
iris %>% 
  ggplot(aes(x = Sepal.Length, y = Petal.Length, colour = Species)) + 
  geom_point() +
  scale_colour_viridis_d() +
  theme_minimal() +
  ggtitle("Lengths")

iris %>% 
  ggplot(aes(x = Sepal.Width, y = Petal.Width, colour = Species)) + 
  geom_point() +
  scale_colour_viridis_d() +
  theme_minimal() +
  ggtitle("Widths")
```

### 9. Fit an additional LDA model, but this time with only Sepal.Length and Sepal.Width as predictors. Call this model lda_iris_sepal

```{r}
lda_iris_sepal <- lda(Species ~ Sepal.Length + Sepal.Width, data = iris)
```

### 10. Create a confusion matrix of the lda_iris and lda_iris_sepal models. (NB: we did not split the dataset into training and test set, so use the training dataset to generate the predictions.). Which performs better in terms of accuracy?
```{r}
# lda_iris
table(true = iris$Species, predicted = predict(lda_iris)$class)

# lda_iris_sepal
table(true = iris$Species, predicted = predict(lda_iris_sepal)$class)
```

# Classification Trees 

### 11. Use rpart() to create a classification tree for the Species of iris. Call this model iris_tree_mod. Plot this model using rpart.plot().

```{r}
iris_tree_mod <- rpart(Species ~ ., data = iris)
rpart.plot(iris_tree_mod)
```

### 12. How would an iris with 2.7 cm long and 1.5 cm wide petals be classified?
```{r}
# just by looking at graph = versicolor
```

### 13. Create a scatterplot where you map Petal.Length to the x position and Petal.Width to the y position. Then, manually add a vertical and a horizontal line (using geom_segment) at the locations of the splits from the classification tree. Interpret this plot.
```{r}
# THIS IS A NICE PLOT
iris %>% 
  ggplot(aes(x = Petal.Length, y = Petal.Width, colour = Species)) +
  geom_point() +
  geom_segment(aes(x = 2.5, xend = 2.5, y = -Inf, yend = Inf),
               colour = "black") +
  geom_segment(aes(x = 2.5, xend = Inf, y = 1.75, yend = 1.75), 
               colour = "black") +
  scale_colour_viridis_d() +
  theme_minimal()
```
```{r}
?rpart.control # tuning parameters
```

### 14. Create a classification tree model where the splits continue until all the observations have been classified. Call this model iris_tree_full_mod. Plot this model using rpart.plot(). Do you expect this model to perform better or worse on new Irises?

```{r}
iris_tree_full_mod <- rpart(Species ~ ., data = iris, 
                            control = rpart.control(minbucket = 1, cp = 0))

rpart.plot(iris_tree_full_mod)

# this model is likely extremely over fit to the data, meaning it will probably preform poorly on new data


```

# Final assignment: Random forest for classification

### 15. Use the function randomForest() to create a random forest model on the iris dataset. Use the function importance() on this model and create a bar plot of variable importance. Does this agree with your expectations? How well does the random forest model perform compared to the lda_iris model?

```{r}
rf_mod<- randomForest(Species~., data = iris)
rf_mod
```

```{r}
# importance(rf_mod)

tibble(
  importance = importance(rf_mod), 
  variable = rownames(importance(rf_mod))
) %>%
  ggplot(aes(x = variable, y = importance, fill = variable)) +
  geom_bar(stat = "identity") 
```
```{r}
# compare with lda iris model 
rf_mod
table(iris$Species, predict(lda_iris)$class) 

# iris is a bit better w.r.t in sample accuracy
```

