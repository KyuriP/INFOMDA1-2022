---
title: "practical4"
author: "Amalia Tsakali"
date: "2022-10-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ISLR)
library(MASS)
library(tidyverse)
```

##Regression in R
1.Create a linear model object called lm_ses using the formula medv ~ lstat and the Boston dataset.
```{r 1}
lm_ses <- lm(formula = medv ~ lstat, data = Boston)
```
2.Use the function coef() to extract the intercept and slope from the lm_ses object. Interpret the slope coefficient.
```{r 2}
coef(lm_ses)
#interpretation: for 1 point increase in lstat the median price drops 0.95 on average
#the intercep is the expected value when lstat=0

```
3.Use summary() to get a summary of the lm_ses object. What do you see? You can use the help file ?summary.lm.

```{r}
summary(lm_ses)

#it gives us the fitted model, the fitted coeeficient as well as the accurasy of those coefficient estimates
```
4.Save the predicted y values to a variable called y_pred
```{r 4}
y_pred<-predict(lm_ses)
```
5.Create a scatter plot with y_pred mapped to the x position and the true y value (Boston$medv) mapped to the y value. What do you see? What would this plot look like if the fit were perfect?
```{r 5}
data.frame(pred=y_pred,obs=Boston$medv) %>% ggplot(aes(x=pred,y=obs)) +geom_point() +theme_minimal()

# if the fit was perfect it would be a straight line

#it looks as if the lm is not the correct model. the difference in the predicted observed is bigger in the big values
```
We can also generate predictions from new data using the newdat argument in the predict() method. For that, we need to prepare a data frame with new values for the original predictors.

6.Use the seq() function to generate a sequence of 1000 equally spaced values from 0 to 40. Store this vector in a data frame with (data.frame() or tibble()) as its column name lstat. Name the data frame pred_dat.
```{r 6}
a<-seq(0,40,length.out = 1000)
pred_dat<-data.frame(lstat=a)
```
7.Use the newly created data frame as the newdata argument to a predict() call for lm_ses. Store it in a variable named y_pred_new.
```{r 7}
y_pred_new<-predict(lm_ses, newdata = pred_dat)
```
##Plotting lm() in ggplot
8.Create a scatter plot from the Boston dataset with lstat mapped to the x position and medv mapped to the y position. Store the plot in an object called p_scatter.

```{r 8}
p_scatter<-Boston %>% ggplot(aes(x=lstat,y=medv)) +geom_point() +theme_minimal()
p_scatter
```

9.Add the vector y_pred_new to the pred_dat data frame with the name medv.
```{r 9}
pred_dat$medv<-y_pred_new
```
10.Add a geom_line() to p_scatter, with pred_dat as the data argument. What does this line represent?
```{r 10}
p_scatter<-ggplot() +geom_point(data = Boston, aes(x=lstat,y=medv))+geom_line(data=pred_dat, aes(x=lstat, y=medv)) +theme_minimal()
p_scatter

#this line represents the predicted medv values for lstat
```
11.The interval argument can be used to generate confidence or prediction intervals. Create a new object called y_pred_95 using predict() (again with the pred_dat data) with the interval argument set to “confidence”. What is in this object?
```{r 11}
y_pred_95<-predict(lm_ses,newdata = pred_dat, interval= "confidence")

head(y_pred_95)
class(y_pred_95)


#its a matrix with upper and lower confidence intervals for the fitted values
```
12.Create a data frame with 4 columns: medv, lstat, lower, and upper.
```{r 12}
y_pred_95<-as.data.frame(y_pred_95)
df_with_confidence<-data.frame(medv=pred_dat$medv, lstat= pred_dat$lstat, lower=y_pred_95$lwr, upper=y_pred_95$upr)
```

13.Add a geom_ribbon() to the plot with the data frame you just made. The ribbon geom requires three aesthetics: x (lstat, already mapped), ymin (lower), and ymax (upper). Add the ribbon below the geom_line() and the geom_points() of before to make sure those remain visible. Give it a nice colour and clean up the plot, too!
```{r 13}
ggplot() +geom_ribbon(data=df_with_confidence,aes(x=lstat,ymin=lower,ymax=upper), fill="deeppink3")+geom_point(data = Boston, aes(x=lstat,y=medv))+geom_line(data=pred_dat, aes(x=lstat, y=medv), colour="purple") +theme_minimal()+ labs(x    = "Proportion of lower status households",y    = "Median house value", title = "Boston house prices")
```
14.Explain in your own words what the ribbon represents.
```{r 14}
#the ribbon represents the 95% confidence intervals of the fitted values

```
15.Do the same thing, but now with the prediction interval instead of the confidence interval.
```{r 15}
y_pred_95_2<-predict(lm_ses,newdata = pred_dat, interval= "prediction")

head(y_pred_95_2)

y_pred_95_2<-as.data.frame(y_pred_95_2)
df_with_prediction<-data.frame(medv=pred_dat$medv, lstat= pred_dat$lstat, lower=y_pred_95_2$lwr, upper=y_pred_95_2$upr)

ggplot() +geom_ribbon(data=df_with_prediction,aes(x=lstat,ymin=lower,ymax=upper), fill="deeppink3")+geom_point(data = Boston, aes(x=lstat,y=medv))+geom_line(data=pred_dat, aes(x=lstat, y=medv), colour="purple") +theme_minimal()+ labs(x    = "Proportion of lower status households",y    = "Median house value", title = "Boston house prices")

#the prediction intervals are always bigger than the confidence intervals (because they are for one observation)
```
##Mean square error
16.Write a function called mse() that takes in two vectors: true y values and predicted y values, and which outputs the mean square error.
```{r 16}
mse <- function(y_true, y_pred) {
  mean((y_true - y_pred)^2)
}
```
17.Make sure your mse() function works correctly by running the following code.
```{r 17}
mse(1:10, 10:1)
```
18.Calculate the mean square error of the lm_ses model. Use the medv column as y_true and use the predict() method to generate y_pred.
```{r 18}
mse(Boston$medv,predict(lm_ses))
```
##Train-validation-test split

19.The Boston dataset has 506 observations. Use c() and rep() to create a vector with 253 times the word “train”, 152 times the word “validation”, and 101 times the word “test”. Call this vector splits.

```{r 19}
splits<-c(rep("train",253),rep("validation",152),rep("test",101))
```

20.Use the function sample() to randomly order this vector and add it to the Boston dataset using mutate(). Assign the newly created dataset to a variable called boston_master.

```{r 20}
boston_master<-Boston %>% mutate(split=sample(splits))
```
21.Now use filter() to create a training, validation, and test set from the boston_master data. Call these datasets boston_train, boston_valid, and boston_test.
```{r}
boston_train<-boston_master %>% filter(split=="train")
boston_valid<-boston_master %>% filter(split=="validation")
boston_test<-boston_master %>% filter(split=="test")
```
22.Train a linear regression model called model_1 using the training dataset. Use the formula medv ~ lstat like in the first lm() exercise. Use summary() to check that this object is as you expect.
```{r 22}
model_1<-lm(medv ~ lstat, boston_train)
summary(model_1)
```
23.Calculate the MSE with this object. Save this value as model_1_mse_train.
```{r 23}
model_1_mse_train<-mse(y_true=boston_train$medv,y_pred=predict(model_1))
```
24.Now calculate the MSE on the validation set and assign it to variable model_1_mse_valid. Hint: use the newdata argument in predict().
```{r 24}
model_1_mse_valid<-mse(y_true = boston_valid$medv,y_pred = predict(model_1,newdata = boston_valid))
```
25.Create a second model model_2 for the train data which includes age and tax as predictors. Calculate the train and validation MSE.
```{r 25}
model_2<-lm(medv ~ lstat+age+tax, boston_train)
summary(model_2)

model_2_mse_train<-mse(y_true=boston_train$medv,y_pred=predict(model_2))
model_2_mse_valid<-mse(y_true = boston_valid$medv,y_pred = predict(model_2,newdata = boston_valid))
```
26.Compare model 1 and model 2 in terms of their training and validation MSE. Which would you choose and why?
```{r 26}
#model 1 has mse for train in validation 37.6 and 43.7 respectivelly
#model 2 has mse for train in validation 34.7 and 43.6 respectivelly
#so 2 is slightly better (in the validation we only have 0.1 difference)

```
27.Calculate the test MSE for the model of your choice in the previous question. What does this number tell you?
```{r 27}
model_2_mse_test<-mse(y_true = boston_test$medv,y_pred = predict(model_2,newdata = boston_test))

#this number (33.66408) tells me the average squared error of my model predictions in unseen data(test dataset)
```
##Programming exercise: cross-validation

28.Create a function that performs k-fold cross-validation for linear models.


```{r 28}
kfold<-function(formula,dataset,k){
 
dataset$fold<-sample(factor(rep(1:k, length.out=nrow(dataset))))
#vector for the mses
mses <- rep(0, k)
for(i in 1:k){
  data_train<-dataset %>% filter(!fold%in% i)
  data_val<- dataset %>% filter(fold %in% i)
  model_i <- lm(formula = formula, data = data_train)
  # Extract the y column name from the formula
  y_column_name <- as.character(formula)[2] #
  mses[i] <- mse(y_true = data_val[[y_column_name]],
                   y_pred = predict(model_i, newdata = data_val))
    
  out <- paste0("Test MSE of iteration ", i, ".")  
  print(out)
  print((mses[i]))
  
  
}
print("Mean MSE value for model")
mean(mses)
}



```


29.Use your function to perform 9-fold cross validation with a linear model with as its formula medv ~ lstat + age + tax. Compare it to a model with as formulat medv ~ lstat + I(lstat^2) + age + tax.
```{r 29}
kfold(medv ~ lstat + age + tax,Boston,9)
kfold(medv ~ lstat + I(lstat^2) + age + tax,Boston,9)

#the second model has significantly lower mse 

```

