---
title: "Practical 4"
author: "Hsuan Lee"
output: html_document
---

```{r}
library(ISLR)
library(MASS)
library(tidyverse)
```

## Regression in R

**1. Create a linear model object called lm_ses using the formula medv ~ lstat and the Boston dataset.**
```{r}
lm_ses <- lm(medv~ lstat, data = Boston)
```

**2. Use the function coef() to extract the intercept and slope from the lm_ses object. Interpret the slope coefficient**
```{r}
coef(lm_ses)
```

**3. Use summary() to get a summary of the lm_ses object. What do you see? You can use the help file ?summary.lm .**
```{r}
summary(lm_ses)
```

**4. Save the predicted y values to a variable called y_pred**
```{r}
y_pred <- lm_ses$fitted.values
Boston_pred <- cbind(Boston, y_pred)
```

**5. Create a scatter plot with y_pred mapped to the x position and the true y value ( Boston$medv ) mapped to the y value. What do you see? What would this plot look like if the fit were perfect?**
```{r}
Boston_pred %>%
  ggplot(aes(x = y_pred, y = medv)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)
```

**6. Use the seq() function to generate a sequence of 1000 equally spaced values from 0 to 40. Store this vector in a data frame with ( data.frame() or tibble() ) as its column name lstat . Name the data frame pred_dat .**
```{r}
pred_dat <- data.frame(lstat = seq(0, 40, length.out = 1000))
```

**7. Use the newly created data frame as the new data argument to a predict() call for lm_ses . Store it in a variable named y_pred_new .**
```{r}
# predicted values
y_pred_new <- predict(lm_ses, newdata = pred_dat)
```

## Plotting lm() in ggplot

**8. Create a scatter plot from the Boston dataset with lstat mapped to the x position and medv mapped to the y position. Store the plot in an object called p_scatter .**
```{r}
p_scatter <- Boston %>%
  ggplot(aes(x = lstat, y = medv)) +
  geom_point()

p_scatter
```

**9. Add the vector y_pred_new to the pred_dat data frame with the name medv .**
```{r}
pred_dat <- pred_dat %>%
  bind_cols(medv = y_pred_new)
head(pred_dat)
```

**10. Add a geom_line() to p_scatter , with pred_dat as the data argument. What does this line represent?**
```{r}
p_scatter +
  geom_line(data = pred_dat)
```

The line is the predicted values.

**11. The interval argument can be used to generate confidence or prediction intervals. Create a new object called y_pred_95 using predict() (again with the pred_dat data) with the interval argument set to “confidence”. What is in this object?**
```{r}
y_pred_95 <- data.frame(predict(lm_ses, newdata = pred_dat, interval = "confidence"))
head(y_pred_95)
```

**12. Create a data frame with 4 columns: medv , lstat , lower , and upper .**
```{r}
df <- data.frame(medv = y_pred_95$fit, lstat = pred_dat$lstat, lower = y_pred_95$lwr, upper = y_pred_95$upr)
```

**13. Add a geom_ribbon() to the plot with the data frame you just made. The ribbon geom requires three aesthetics: x ( lstat , already mapped), ymin ( lower ), and ymax ( upper ). Add the ribbon below the geom_line() and the geom_points() of before to make sure those remain visible.Give it a nice colour and clean up the plot, too!**
```{r}
Boston %>%
  ggplot(aes(x = lstat, y = medv)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), data = df, fill = "blue") +
  geom_point() +
  geom_line(data = pred_dat, color = 'red')
```

**14. Explain in your own words what the ribbon represents.**

The ribbon represents the 95% confidence interval of the predicted linear.

**15. Do the same thing, but now with the prediction interval instead of the confidence interval.**
```{r}
y_predicted_95 <- data.frame(predict(lm_ses, newdata = pred_dat, interval = "prediction"))

df_pre <- data.frame(medv = y_predicted_95$fit, lstat = pred_dat$lstat, lower = y_predicted_95$lwr, upper = y_predicted_95$upr)

Boston %>%
  ggplot(aes(x = lstat, y = medv)) +
  geom_point() +
  geom_line(data = df_pre, color = "dark blue") +
  geom_ribbon(data = df_pre, aes(ymin = lower, ymax = upper), fill = "#00008b44")
```

## Mean square error

**16. Write a function called mse() that takes in two vectors: true y values and predicted y values, and which outputs the mean square error.**
```{r}
mse <- function(y_true, y_pred){
  mse = mean((y_pred - y_true)^2)
  return(mse)
}
```

**17. Make sure your mse() function works correctly by running the following code.**
```{r}
mse(1:10, 10:1)
```

**18. Calculate the mean square error of the lm_ses model. Use the medv column as y_true and use the predict() method to generate y_pred .**
```{r}
mse(y_true = Boston$medv, y_pred = lm_ses$fitted.values)
```

## Train-validation-test split

**19. The Boston dataset has 506 observations. Use c() and rep() to create a vector with 253 times the word “train”, 152 times the word“validation”, and 101 times the word “test”. Call this vector splits .**
```{r}
split <- c(rep('train', 253), rep("validation", 152), rep("test", 101))
```

**20. Use the function sample() to randomly order this vector and add it to the Boston dataset using mutate() . Assign the newly created dataset to a variable called boston_master .**
```{r}
boston_master <- Boston %>%
  mutate(split = sample(split))
```

**21. Now use filter() to create a training, validation, and test set from the boston_master data. Call these datasets boston_train , boston_valid ,and boston_test .**
```{r}
boston_train <- boston_master %>%
  filter(split == "train")
boston_valid <- boston_master %>%
  filter(split == 'validation')
boston_test <- boston_master %>%
  filter(split == 'test')
```

**22. Train a linear regression model called model_1 using the training dataset. Use the formula medv ~ lstat like in the first lm() exercise. Use summary() to check that this object is as you expect.**
```{r}
model_1 <- lm(medv~ lstat, data = boston_train)
summary(model_1)
```

**23. Calculate the MSE with this object. Save this value as model_1_mse_train .**
```{r}
model_1_mse_train <- mse(boston_train$medv, model_1$fitted.values)
model_1_mse_train
```

**24. Now calculate the MSE on the validation set and assign it to variable model_1_mse_valid . Hint: use the newdata argument in predict() .**
```{r}
vali <- predict(model_1, newdata = boston_valid)
model_1_mse_valid <- mse(boston_valid$medv, vali)
model_1_mse_valid
```

**25. Create a second model model_2 for the train data which includes age and tax as predictors. Calculate the train and validation MSE.**
```{r}
model_2 <- lm(medv~ lstat + age + tax, data = boston_train)
model_2_mse_train <- mse(boston_train$medv, model_2$fitted.values)
model_2_mse_train

model_2_mse_valid <- mse(boston_valid$medv,
                         predict(model_2, newdata = boston_valid))
model_2_mse_valid
```

**26. Compare model 1 and model 2 in terms of their training and validation MSE. Which would you choose and why?**

Because of the addition of the new predictors in model 2, the models fit the data better, MSE turn lower. In general, one predictor is not enough to build a effective model. However, we should follow the theory, to decide whether include the new predictors make sense, in our case, i prefer model 2. 

**27. Calculate the test MSE for the model of your choice in the previous question. What does this number tell you?**
```{r}
model_2_mse_test <- mse(boston_test$medv,
                        predict(model_2, newdata = boston_test))
model_2_mse_test
```

## Programming exercise: cross-validation

**28. Create a function that performs k-fold cross-validation for linear models.**
```{r}
k_fold_cv <- function(formula, data, k){
  n_samples  <- nrow(data)
  select_vec <- rep(1:k, length.out = n_samples)
  data_split <- data %>% mutate(folds = sample(select_vec))
  
  mses <- rep(0, k)
  
  for (i in 1:k) {
    data_train <- data_split %>% filter(folds != i)
    data_valid <- data_split %>% filter(folds == i)
    
    model_i <- lm(formula = formula, data = data_train)
    
    y_column_name <- as.character(formula)[2]
    
    mses[i] <- mse(y_true = data_valid[[y_column_name]],
                   y_pred = predict(model_i, newdata = data_valid))
  }
   mean(mses)
}
```

**29. Use your function to perform 9-fold cross validation with a linear model with as its formula medv ~ lstat + age + tax. Compare it to a model with as formulat medv ~ lstat + I(lstat^2) + age + tax.**
```{r}
k_fold_cv(formula = medv ~ lstat + age + tax, data = Boston, k = 9)
```

```{r}
k_fold_cv(formula = medv ~ lstat + I(lstat^2) + age + tax, data = Boston, k = 9)
```














