---
title: 'Supervised learning: Regression 1'
author: "Ana Martins"
date: "October 2022"
output: html_document
---

## Introduction

```{r}
library(ISLR)
library(MASS)
library(tidyverse)
```

## Regression in `R`

1. **Create a linear model object called `lm_ses` using the formula `medv ~ lstat` and the `Boston` dataset.**

```{r}
lm_ses <- lm(data = Boston, formula = medv ~ lstat)
lm_ses
```

2. **Use the function `coef()` to extract the intercept and slope from the `lm_ses` object. Interpret the slope coefficient.**

```{r}
coef(lm_ses)
```

The median value of owner-occupied homes in $1000s goes down as the lower status of the population goes up.

3. **Use `summary()` to get a summary of the `lm_ses` object. What do you see? You can use the help file `?summary.lm`.**

```{r}
summary(lm_ses)
```

4. **Save the predicted y values to a variable called `y_pred`**

```{r}
y_pred <- predict(lm_ses)
y_pred
```

5. **Create a scatter plot with `y_pred` mapped to the x position and the true y value (`Boston$medv`) mapped to the y value. What do you see? What would this plot look like if the fit were perfect?**

```{r}
ggplot() +
  geom_point(data = Boston, mapping = aes(x = y_pred, y = medv))
```

If the fit were perfect the plot would be a straight line on y = x.

6. **Use the `seq()` function to generate a sequence of 1000 equally spaced values from 0 to 40. Store this vector in a data frame with (`data.frame()` or `tibble()`) as its column name `lstat`. Name the data frame `pred_dat`.**

```{r}
pred_data <- data.frame(lstat = seq(0, 40, length.out = 1000))
pred_data
```

7. **Use the newly created data frame as the `newdata` argument to a `predict()` call for `lm_ses`. Store it in a variable named `y_pred_new`.**

```{r}
y_pred_new <- predict(lm_ses, newdata = pred_data)
y_pred_new
```

## Plotting lm() in `ggplot`

8. **Create a scatter plot from the `Boston` dataset with `lstat` mapped to the x position and `medv` mapped to the y position. Store the plot in an object called `p_scatter`.**

```{r}
p_scatter <-
  ggplot() +
  geom_point(data = Boston, mapping = aes(x = lstat, y = medv))
p_scatter
```

9. **Add the vector `y_pred_new` to the `pred_dat` data frame with the name `medv`.**

```{r}
pred_data <-
  pred_data %>% 
  mutate(medv = y_pred_new)
pred_data
```

10. **Add a geom_line() to `p_scatter`, with `pred_dat` as the `data` argument. What does this line represent?**

```{r}
p_scatter <-
  p_scatter +
  geom_line(data = pred_data, mapping = aes(x = lstat, y = medv))
p_scatter
```

The prediction for the data.

11. **The `interval` argument can be used to generate confidence or prediction intervals. Create a new object called `y_pred_95` using `predict()` (again with the `pred_dat` data) with the `interval` argument set to “confidence”. What is in this object?**

```{r}
y_pred_95 <- predict(lm_ses, newdata = pred_data, interval = "confidence")
y_pred_95
class(y_pred_95)
```

The fitted value for various xs and the lower and upper confidence limit.

12. **Create a data frame with 4 columns: `medv`, `lstat`, `lower`, and `upper`.**

```{r}
df <-
  data.frame(lstat = pred_data$lstat,
             medv = y_pred_95[, 1],
             lower = y_pred_95[, 2],
             upper = y_pred_95[, 3])
df
```

13. **Add a `geom_ribbon()` to the plot with the data frame you just made. The ribbon geom requires three aesthetics: `x` (`lstat`, already mapped), `ymin` (`lower`), and `ymax` (`upper`). Add the ribbon below the `geom_line()` and the `geom_points()` of before to make sure those remain visible. Give it a nice colour and clean up the plot, too!**

```{r}
ggplot(data = Boston, mapping = aes(x = lstat, y = medv)) +
  geom_ribbon(data = df, mapping = aes(ymin = lower, ymax = upper), fill = "#e5f5f9") +
  geom_point(color = "#99d8c9") +
  geom_line(data = pred_data, color = "#2ca25f", size = 1) +
  theme_minimal()
```

14. **Explain in your own words what the ribbon represents.**

The ribbon encloses all values within the confidence interval for the medv variable.


15. **Do the same thing, but now with the prediction interval instead of the confidence interval.**

```{r}
y_pred_int <- predict(lm_ses, newdata = pred_data, interval = "prediction")
y_pred_int

df_int <-
  data.frame(lstat = pred_data$lstat,
             medv = y_pred_int[, 1],
             lower = y_pred_int[, 2],
             upper = y_pred_int[, 3])
df_int
ggplot(data = Boston, mapping = aes(x = lstat, y = medv)) +
  geom_ribbon(data = df_int, mapping = aes(ymin = lower, ymax = upper), fill = "#e0ecf4") +
  geom_point(color = "#9ebcda") +
  geom_line(data = pred_data, color = "#8856a7") +
  theme_minimal()
```


## Mean square error

16. **Write a function called `mse()` that takes in two vectors: true y values and predicted y values, and which outputs the mean square error.**

```{r}
mse <- function(y_true, y_pred) {
  mean((y_true - y_pred)^2)
}
```

17. **Make sure your `mse()` function works correctly by running the following code.**

```{r}
mse(1:10, 10:1)
```

18. **Calculate the mean square error of the `lm_ses` model. Use the `medv` column as `y_true` and use the `predict()` method to generate `y_pred`.**

```{r}
mse(Boston$medv, predict(lm_ses))
```


## Train-validation-test split

19. **The `Boston` dataset has 506 observations. Use `c()` and `rep()` to create a vector with 253 times the word “train”, 152 times the word “validation”, and 101 times the word “test”. Call this vector `splits`.**

```{r}
splits <- c(rep("train", 253), rep("validation", 152), rep("test", 101))
```

20. **Use the function `sample()` to randomly order this vector and add it to the `Boston` dataset using `mutate()`. Assign the newly created dataset to a variable called `boston_master`.**

```{r}
boston_master <-
  Boston %>% 
  mutate(splits = sample(splits))
```

21. **Now use `filter()` to create a training, validation, and test set from the `boston_master` data. Call these datasets `boston_train`, `boston_valid`, and `boston_test`.**

```{r}
boston_train <-
  boston_master %>% 
  filter(splits == "train")

boston_valid <-
  boston_master %>% 
  filter(splits == "validation")

boston_test <-
  boston_master %>% 
  filter(splits == "test")
```

22. **Train a linear regression model called `model_1` using the training dataset. Use the formula `medv ~ lstat` like in the first `lm()` exercise. Use `summary()` to check that this object is as you expect.**

```{r}
model_1 <- lm(medv ~ lstat, data = boston_train)
summary(model_1)
```

23. **Calculate the MSE with this object. Save this value as `model_1_mse_train`.**

```{r}
model_1_mse_train <- mse(y_true = boston_train$medv, y_pred = predict(model_1))
model_1_mse_train
```

24. **Now calculate the MSE on the validation set and assign it to variable `model_1_mse_valid`. Hint: use the `newdata` argument in `predict()`.**

```{r}
model_1_mse_valid <- mse(y_true = boston_valid$medv, 
                         y_pred = predict(model_1, newdata = boston_valid))
model_1_mse_valid
```

25. **Create a second model `model_2` for the train data which includes `age` and `tax` as predictors. Calculate the train and validation MSE.**

```{r}
model_2 <- lm(medv ~ lstat + age + tax, data = boston_train)
model_2_mse_train <- mse(y_true = boston_train$medv, y_pred = predict(model_2))
model_2_mse_valid <- mse(y_true = boston_valid$medv, 
                         y_pred = predict(model_2, newdata = boston_valid))
```


## Programming exercise: cross-validation